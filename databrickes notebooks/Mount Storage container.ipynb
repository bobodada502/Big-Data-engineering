{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa8a75e1-ea9b-47b1-bbf8-1133c52eaed8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Mount Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f547c20-931d-4a61-ac7c-d7543e6c1a91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storageAccountName = 'primumstorage01'\n",
    "containerName = 'bd-project'\n",
    "applicationId = 'applicationId'\n",
    "directoryID = 'directoryID'\n",
    "secretValue = 'secretValue'\n",
    "endpoint = 'https://login.microsoftonline.com/' + directoryID + '/oauth2/token'\n",
    "source = 'abfss://' + containerName + '@' + storageAccountName + '.dfs.core.windows.net/'\n",
    "mountPoint = \"/mnt/my_data\"\n",
    "\n",
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "           \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "           \"fs.azure.account.oauth2.client.id\": applicationId,\n",
    "           \"fs.azure.account.oauth2.client.secret\": secretValue,\n",
    "           \"fs.azure.account.oauth2.client.endpoint\": endpoint}\n",
    "\n",
    "\n",
    "dbutils.fs.mount(source = source,mount_point = mountPoint, extra_configs = configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72f1f2ae-5ef2-44a4-b76b-e3b86ac2cc0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/my_data has been unmounted.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.unmount(\"/mnt/my_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbd55e04-e102-4572-8c83-3616663bbee2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/my_data/Users/\"raw_st\".\"users\".csv</td><td>\"raw_st\".\"users\".csv</td><td>242846485</td><td>1715171646000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/mnt/my_data/Users/\"raw_st\".\"users\".csv",
         "\"raw_st\".\"users\".csv",
         242846485,
         1715171646000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "  dbutils.fs.ls(\"/mnt/my_data/Users\")\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e506325b-9c0d-4309-93c7-4f450ac3a14c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MountInfo(mountPoint='/databricks-datasets', source='databricks-datasets', encryptionType=''),\n",
       " MountInfo(mountPoint='/Volumes', source='UnityCatalogVolumes', encryptionType=''),\n",
       " MountInfo(mountPoint='/mnt/my_data', source='abfss://bd-project@primumstorage01.dfs.core.windows.net/', encryptionType=''),\n",
       " MountInfo(mountPoint='/databricks/mlflow-tracking', source='databricks/mlflow-tracking', encryptionType=''),\n",
       " MountInfo(mountPoint='/databricks-results', source='databricks-results', encryptionType=''),\n",
       " MountInfo(mountPoint='/databricks/mlflow-registry', source='databricks/mlflow-registry', encryptionType=''),\n",
       " MountInfo(mountPoint='/Volume', source='DbfsReserved', encryptionType=''),\n",
       " MountInfo(mountPoint='/volumes', source='DbfsReserved', encryptionType=''),\n",
       " MountInfo(mountPoint='/', source='DatabricksRoot', encryptionType=''),\n",
       " MountInfo(mountPoint='/volume', source='DbfsReserved', encryptionType='')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " dbutils.fs.mounts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d717ab16-3db3-4ee0-aea7-180987784605",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify if the data is mounted correctly\n",
    "\n",
    "expected_source = 'abfss://' + containerName + '@' + storageAccountName + '.dfs.core.windows.net/'\n",
    "\n",
    "if source == expected_source:\n",
    "    print(\"Data is mounted correctly\")\n",
    "else:\n",
    "    print(\"Data is not mounted correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4dbf8e9-881a-42f8-95cf-eb233f65640b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##  Machine Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dce1d308-82bd-46b5-abdd-0b3e133c32e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Table Loading\")\n",
    "         .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e54880b-a9ea-4abe-8537-0f07044f41a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_location = \"/mnt/my_data/ml_training/Posts/*\"\n",
    "\n",
    "posts = spark.read \\\n",
    "  .parquet(file_location)\n",
    "\n",
    "# display(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c169dd5-8f7a-4e46-9ba4-e92c4961d268",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "722c5419-a830-498a-a5d7-d86eefba07ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('id', IntegerType(), True), StructField('Type', StringType(), True)])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating the schema for posttypes table\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "PT_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"Type\", StringType(), True)\n",
    "])\n",
    "display(PT_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4beeab99-0213-4bc3-bf58-574a10cb7c71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating the posttypes dataframe\n",
    "file_location = \"/mnt/my_data/ml_training/PostTypes.txt\"\n",
    "\n",
    "postType = (spark.read\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"sep\", \",\")\n",
    "  .schema(PT_schema)\n",
    "  .csv(file_location))\n",
    "\n",
    "display(postType)\n",
    "type(postType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dae71e84-de4a-4232-9a4c-ed8d15b7f3c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('id', IntegerType(), True), StructField('Age', IntegerType(), True), StructField('CreationDate', DateType(), True), StructField('DisplayName', StringType(), True), StructField('DownVotes', IntegerType(), True), StructField('EmailHash', StringType(), True), StructField('Location', StringType(), True), StructField('Reputation', IntegerType(), True), StructField('UpVotes', IntegerType(), True), StructField('Views', IntegerType(), True), StructField('WebsiteUrl', StringType(), True), StructField('AccountId', IntegerType(), True)])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating the schema for the users table\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "users_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"CreationDate\", DateType(), True),\n",
    "    StructField(\"DisplayName\", StringType(), True),\n",
    "    StructField(\"DownVotes\", IntegerType(), True),\n",
    "    StructField(\"EmailHash\", StringType(), True),\n",
    "    StructField(\"Location\", StringType(), True),\n",
    "    StructField(\"Reputation\", IntegerType(), True),\n",
    "    StructField(\"UpVotes\", IntegerType(), True),\n",
    "    StructField(\"Views\", IntegerType(), True),\n",
    "    StructField(\"WebsiteUrl\", StringType(), True),\n",
    "    StructField(\"AccountId\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "display(users_schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43d5c06f-458c-4dc2-8ff1-e2e3b12d5d25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating the users dataframe\n",
    "file_location = \"/mnt/my_data/ml_training/users.csv\"\n",
    "\n",
    "users = (spark.read\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"sep\", \",\")\n",
    "  .schema(users_schema)\n",
    "  .csv(file_location))\n",
    "\n",
    "display(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07a4855a-f672-40b7-8118-f88ef3ccc38f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the 3 tables to databricks local file system\n",
    "posts.write.parquet(\"/tmp/project/posts.parquet\")\n",
    "postType.write.parquet(\"/tmp/project/PostType.parquet\")\n",
    "users.write.parquet(\"/tmp/project/user.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ca2e69a-9dd5-4c6f-b34b-3e7ca1e5e357",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/tmp/project/PostType.parquet/</td><td>PostType.parquet/</td><td>0</td><td>1715527670000</td></tr><tr><td>dbfs:/tmp/project/posts.parquet/</td><td>posts.parquet/</td><td>0</td><td>1715527666000</td></tr><tr><td>dbfs:/tmp/project/user.parquet/</td><td>user.parquet/</td><td>0</td><td>1715527671000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/tmp/project/PostType.parquet/",
         "PostType.parquet/",
         0,
         1715527670000
        ],
        [
         "dbfs:/tmp/project/posts.parquet/",
         "posts.parquet/",
         0,
         1715527666000
        ],
        [
         "dbfs:/tmp/project/user.parquet/",
         "user.parquet/",
         0,
         1715527671000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# review the local file system\n",
    "display(dbutils.fs.ls(\"/tmp/project/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0d00739-423a-498d-bae1-87d62715313d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries and functions\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, translate, trim, explode, regexp_replace, col, lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29df5b66-4ba6-458f-8512-8545eefbe22c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating Spark Session\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"ML Model\")\n",
    "         .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c466c98-6e74-44f5-a5a8-95dd77b4e666",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "posts = spark.read.parquet(\"/tmp/project/posts.parquet\")\n",
    "postType = spark.read.parquet(\"/tmp/project/PostType.parquet\")\n",
    "Users = spark.read.parquet(\"/tmp/project/user.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40631f72-3b85-409b-a0c8-b51820de36f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# at this moment, we only use Posts and posttypes to train the model. so let's join them iwith the posttype id. \n",
    "\n",
    "df= posts.join(postType, posts.PostTypeId == postType.id)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edb2fa44-73d1-494f-8a69-dafe9d052bb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter the dataframe to only include questions\n",
    "df= df.filter(col(\"Type\") == \"Question\")\n",
    "# display(df1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5226ae64-a1c1-475c-ad6a-791d9013ba17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (df.withColumn('Body', regexp_replace(df.Body, r'<.*?>', '')) # Transforming HTML code to strings\n",
    "      .withColumn(\"Tags\", split(trim(translate(col(\"Tags\"), \"<>\", \" \")), \" \")) # Making a list of the tags\n",
    ")\n",
    "\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "756ebd0b-8b22-44c6-a382-80280976ff9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter the dataframe to only include questions\n",
    "df = df.filter(col(\"Type\") == \"Question\")\n",
    "# display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f2c4cb5-3c8f-4fe4-9a96-5188a794cc58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.select(col(\"Body\").alias(\"text\"), col(\"Tags\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efaf5318-26e4-4e75-b49d-5139d57ecb1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Producing the tags as individual tags instead of an array\n",
    "# This is duplicating the posts for each possible tag\n",
    "df = df.select(\"text\", explode(\"Tags\").alias(\"tags\"))\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45fee08b-71d3-40e6-8c19-27a66d505cbe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "---------------------------------- Now to clean text column from DataFrame we created----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9b777f0-2e0c-422e-8e60-5d0f52c3590c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# saving the file as a checkpoint (in case the cluster gets terminated)\n",
    "\n",
    "df.write.parquet(\"/tmp/project.df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea75932-b73a-4d0f-aac3-46b8b8ed2631",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2697"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the dataframe to memory for repetitive use\n",
    "df.cache()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76175e76-1ce6-407a-9fde-123b4a6225c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing the data\n",
    "cleaned =         df.withColumn('text', regexp_replace('text', r\"http\\S+\", \"\")) \\\n",
    "                    .withColumn('text', regexp_replace('text', r\"[^a-zA-z]\", \" \")) \\\n",
    "                    .withColumn('text', regexp_replace('text', r\"\\s+\", \" \")) \\\n",
    "                    .withColumn('text', lower('text')) \\\n",
    "                    .withColumn('text', trim('text')) \n",
    "# display(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d0e3577-e404-4049-9907-afcc3e3ff2ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df) # to understand what happend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd51e68f-d6cb-487d-ad1e-d8bbd9d08fa5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e84afe1b-ac86-41f7-939d-388ca935fa60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "tokenized = tokenizer.transform(cleaned)\n",
    "\n",
    "# display(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c42ef87d-101d-473a-995c-d66951cdeffb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "stopword_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "stopword = stopword_remover.transform(tokenized)\n",
    "\n",
    "# display(stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2341ae5d-5ba0-4b6c-8155-221694bb209a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"filtered\", outputCol='cv')\n",
    "cv_model = cv.fit(stopword)\n",
    "text_cv = cv_model.transform(stopword)\n",
    "\n",
    "# display(text_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f711b212-d6f7-4d2d-9e49-916836da9749",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "idf_model = idf.fit(text_cv)\n",
    "text_idf = idf_model.transform(text_cv)\n",
    "\n",
    "# display(text_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b9b00e7-1156-4c7c-951d-4329c1af3523",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(text_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff8c71df-81d9-4309-8f42-47e23c25564a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###  Note: i had a dataframe and every time i put a new column, depend it on previous column, but with clean that column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3480ad04-32cf-40e8-9af8-1c346ff87f8f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "------------------------------------ i had cleaned the text column and now clean the tag column-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab56d28d-5cae-4a1e-b0e0-413c8def1f21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "label_encoder = StringIndexer(inputCol = \"tags\", outputCol = \"label\")\n",
    "le_model = label_encoder.fit(text_idf)\n",
    "final = le_model.transform(text_idf)\n",
    "\n",
    "# display(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66ae9e56-bef9-442a-9e2e-956f644aa6e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-755696554403870>, line 1\u001b[0m\n",
       "\u001b[0;32m----> 1\u001b[0m display(\u001b[43mfinal\u001b[49m)\n",
       "\n",
       "\u001b[0;31mNameError\u001b[0m: name 'final' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "NameError",
        "evalue": "name 'final' is not defined"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'final' is not defined"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "File \u001b[0;32m<command-755696554403870>, line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m display(\u001b[43mfinal\u001b[49m)\n",
        "\u001b[0;31mNameError\u001b[0m: name 'final' is not defined"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2211578-d84e-4d95-be50-85e1cfc82a52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72204e05f2734c8b9f39095e038655c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14358c88ef974ea09ac99cc9e1088879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "\n",
    "lr_model = lr.fit(final)\n",
    "\n",
    "predictions = lr_model.transform(final)\n",
    "\n",
    "# display(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21f07b91-aa3d-475b-b346-a9391ef51288",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55700254-d81f-45ee-8f77-144ab1af4f0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.3482\n",
      "ROC-AUC: 0.2812\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(predictions.count())\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "769a5c49-5aab-4669-a3d2-6ad1abecc488",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "------------------------------------------------ALL Codes in one Cell-------------------------------------------------------\n",
    "\n",
    "### Importing all the libraries\n",
    "from pyspark.sql.functions import split, translate, trim, explode, regexp_replace, col, lower\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "## Preparing the data\n",
    "### Step 1: Creating the joined table\n",
    "df = posts.join(postType, posts.PostTypeId == postType.id)\n",
    "### Step 2: Selecting only Question posts\n",
    "df = df.filter(col(\"Type\") == \"Question\")\n",
    "### Step 3: Formatting the raw data\n",
    "df = (df.withColumn('Body', regexp_replace(df.Body, r'<.*?>', ''))\n",
    "      .withColumn(\"Tags\", split(trim(translate(col(\"Tags\"), \"<>\", \" \")), \" \"))\n",
    ")\n",
    "### Step 4: Selecting the columns\n",
    "df = df.select(col(\"Body\").alias(\"text\"), col(\"Tags\"))\n",
    "### Step 5: Getting the tags\n",
    "df = df.select(\"text\", explode(\"Tags\").alias(\"tags\"))\n",
    "### Step 6: Clean the text\n",
    "cleaned = df.withColumn('text', regexp_replace('text', r\"http\\S+\", \"\")) \\\n",
    "                    .withColumn('text', regexp_replace('text', r\"[^a-zA-z]\", \" \")) \\\n",
    "                    .withColumn('text', regexp_replace('text', r\"\\s+\", \" \")) \\\n",
    "                    .withColumn('text', lower('text')) \\\n",
    "                    .withColumn('text', trim('text')) \n",
    "\n",
    "## Machine Learning\n",
    "### Step 1: Train Test Split\n",
    "train, test = cleaned.randomSplit([0.9, 0.1], seed=20200819)\n",
    "### Step 2: Initializing the transfomers\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "stopword_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"filtered\", outputCol='cv')\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5)\n",
    "label_encoder = StringIndexer(inputCol = \"tags\", outputCol = \"label\")\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "### Step 3: Creating the pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, stopword_remover, cv, idf, label_encoder, lr])\n",
    "### Step 4: Fitting and transforming (predicting) using the pipeline\n",
    "pipeline_model = pipeline.fit(train)\n",
    "predictions = pipeline_model.transform(test)\n",
    "------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55eecede-2343-4e2e-8ef6-ac52d888cd3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8ed6bbda824bfabbfff62ae0477b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d6a2fd34fb4859be0345b04f138c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing all the libraries\n",
    "from pyspark.sql.functions import split, translate, trim, explode, regexp_replace, col, lower\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Preparing the data\n",
    "# Step 1: Creating the joined table\n",
    "df = posts.join(postType, posts.PostTypeId == postType.id)\n",
    "# Step 2: Selecting only Question posts\n",
    "df = df.filter(col(\"Type\") == \"Question\")\n",
    "# Step 3: Formatting the raw data\n",
    "df = (df.withColumn('Body', regexp_replace(df.Body, r'<.*?>', ''))\n",
    "      .withColumn(\"Tags\", split(trim(translate(col(\"Tags\"), \"<>\", \" \")), \" \"))\n",
    ")\n",
    "# Step 4: Selecting the columns\n",
    "df = df.select(col(\"Body\").alias(\"text\"), col(\"Tags\"))\n",
    "# Step 5: Getting the tags\n",
    "df = df.select(\"text\", explode(\"Tags\").alias(\"tags\"))\n",
    "# Step 6: Clean the text\n",
    "cleaned = df.withColumn('text', regexp_replace('text', r\"http\\S+\", \"\")) \\\n",
    "                    .withColumn('text', regexp_replace('text', r\"[^a-zA-z]\", \" \")) \\\n",
    "                    .withColumn('text', regexp_replace('text', r\"\\s+\", \" \")) \\\n",
    "                    .withColumn('text', lower('text')) \\\n",
    "                    .withColumn('text', trim('text')) \n",
    "\n",
    "# Machine Learning\n",
    "# Step 1: Train Test Split\n",
    "train, test = cleaned.randomSplit([0.9, 0.1], seed=20200819)\n",
    "# Step 2: Initializing the transfomers\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "stopword_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"filtered\", outputCol='cv')\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5)\n",
    "label_encoder = StringIndexer(inputCol = \"tags\", outputCol = \"label\")\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "# Step 3: Creating the pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, stopword_remover, cv, idf, label_encoder, lr])\n",
    "# Step 4: Fitting and transforming (predicting) using the pipeline\n",
    "pipeline_model = pipeline.fit(train)\n",
    "predictions = pipeline_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c2f1bb2-7918-41e5-970f-dbd714a9e8de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Saving model object to the /mnt/deBDProject directory. Yours name may be different.\n",
    "pipeline_model.save('/mnt/my_data/model')\n",
    "\n",
    "# Save the the String Indexer to decode the encoding. We need it in the future Sentiment Analysis.\n",
    "le_model.save('/mnt/my_data/stringindexer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f65239ad-479f-4168-b479-d1663d0d80f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/my_data/model/metadata/</td><td>metadata/</td><td>0</td><td>1715590248000</td></tr><tr><td>dbfs:/mnt/my_data/model/stages/</td><td>stages/</td><td>0</td><td>1715590250000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/mnt/my_data/model/metadata/",
         "metadata/",
         0,
         1715590248000
        ],
        [
         "dbfs:/mnt/my_data/model/stages/",
         "stages/",
         0,
         1715590250000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/my_data/stringindexer/data/</td><td>data/</td><td>0</td><td>1715590292000</td></tr><tr><td>dbfs:/mnt/my_data/stringindexer/metadata/</td><td>metadata/</td><td>0</td><td>1715590289000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/mnt/my_data/stringindexer/data/",
         "data/",
         0,
         1715590292000
        ],
        [
         "dbfs:/mnt/my_data/stringindexer/metadata/",
         "metadata/",
         0,
         1715590289000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls(\"/mnt/my_data/model\"))\n",
    "display(dbutils.fs.ls(\"/mnt/my_data/stringindexer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b6fab02-6583-4555-8484-e97c2cbde2b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Mount Storage container",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
